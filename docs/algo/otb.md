# Optimal Token Baseline (OTB)

Last updated: 02/23/2026.

ğŸ“ [ArXiv](https://www.arxiv.org/abs/2602.07078)Â | ğŸ“’ [Blog](https://richardli.xyz/optimal-token-baseline) |Â ğŸ¤— [Datasets](https://huggingface.co/datasets/Jiawei415/DPAO_filter) 

Optimal Token Baseline (OTB) is a dynamic token-level baseline for gradient variance reduction in policy-gradient reinforcement learning. It weights updates with the "Realized Energy" statistic that tracks how much uncertainty has accumulated up to each token, so noisy regions get downweighted while confident regions carry more weight.

## Key properties

- _Token-level baselines:_ OTB adapts per token by tracking realized energy, avoiding the padding artifacts that appear when group means dilute the signal with `EOS` tokens.
- _Forward-only overhead:_ The realized-energy statistic is computed via the **Logit-Gradient Proxy**, so OTB requires no extra backward passes or gradient-norm kernels.

## Logit-Gradient Proxy

Computing true uncertainty per token would normally mandate per-token backward passes. OTB sidesteps this by estimating realized energy entirely from forward probabilities, so it introduces negligible runtime overhead in practice.

## Mechanics at a glance

For each prompt group of size `N`, OTB computes rewards-to-go `G_t` and cumulative variance weights `W_t`. The optimal baseline per token is

```
B*_t = (Î£_i G_t^{(i)} Â· W_t^{(i)}) / (Î£_i W_t^{(i)} + Îµ),
W_t = Î£_{j=1}^t (1 - 2Ï€_j + Î£Ï€_jÂ²),
Î£Ï€_jÂ² = exp(logsumexp(2Â·logits_j) - 2Â·logsumexp(logits_j)).
```

The final advantage is `(G_t - B*_t) Â· mask_t`, so padding tokens stay at zero.

## Integration in VERL

- `AdvantageEstimator.OPTIMAL_TOKEN_BASELINE` registers `compute_optimal_token_baseline_advantage`, invoked whenever `algorithm.adv_estimator` is set to `optimal_token_baseline`.
- `ActorRolloutRefWorker.compute_log_prob` emits an additional tensor `sum_pi_squared` (Î£Ï€Â² per token) when `actor.calculate_sum_pi_squared=True`. This requires disabling fused log-prob kernels, because they do not surface logits.
- Trainers assert `sum_pi_squared` exists, regroup trajectories by `non_tensor_batch["uid"]`, and run the OTB calculation. If rollout IS is active, they rescale the weights by `rollout_is_weights**2` before aggregating.
- In Ulysses sequence-parallel setups, the actor gathers, unpads, and returns Î£Ï€Â² in the same way it handles log-probabilities, so OTB supports sharded sequence-parallel models out of the box.
- `sum_pi_squared_checkpointing` is available to trade compute for memory when Î£Ï€Â² tensors become large (e.g., lengthy chain-of-thought reasoning).

## Configuration checklist

- `actor_rollout_ref.actor.calculate_sum_pi_squared: true` (mandatory).
- `actor_rollout_ref.model.use_fused_kernels: false` (required until fused kernels emit logits).
- `algorithm.adv_estimator: optimal_token_baseline` for single-turn RL and `tir_optimal_token_baseline` for multi-turn RL.
- Group sampling (`actor_rollout_ref.rollout.n > 1`) to unlock OTBâ€™s variance reduction; with `n=1` the baseline collapses to returns.

Example OmegaConf overlay:

```yaml
algorithm:
  adv_estimator: optimal_token_baseline

actor_rollout_ref:
  actor:
    calculate_sum_pi_squared: true
    sum_pi_squared_checkpointing: false # optional memory saver
  rollout:
    n: 8
```

## Example script

See `examples/otb_trainer/run_qwen2_5-7b.sh` for a reference training loop.

## Gradient Variance Proxy Metrics

All gradient-variance analysis in the Optimal Token Baseline work starts from the variance identity

```
Var(Ä) = E[||Ä||Â²] - ||E[Ä]||Â²,
```

which states that the variance of any stochastic gradient equals the mean squared magnitude minus the squared norm of its expectation.

For a trajectory `Ï„`, the policy-gradient estimator is

```
Ä(Ï„) = âˆ‡ log Ï€_Î¸(Ï„) Â· A(Ï„),        A(Ï„) = R(Ï„) - B.
```

The logit-gradient proxy approximates the squared gradient norm without an extra backward pass:

```
||Ä(Ï„)||Â² â‰ˆ WÌ‚(Ï„) Â· A(Ï„)Â²,
```

where `WÌ‚(Ï„)` is the realized energy built. Given a mini-batch `{Ï„_i}` of size `N`, we decompose its statistics into three diagnostics:

- **Signal strength (squared norm of the mean gradient)**
  ```
  S = || (1/N) Â· Î£ Ä(Ï„_i) ||Â²
  ```
- **Total power (signal + noise)**
  ```
  P_total = (1/N) Â· Î£ WÌ‚(Ï„_i) Â· A(Ï„_i)Â²
  ```
- **Pure noise (estimated variance of the batch mean)**
  ```
  Var_proxy = (1/(N-1)) Â· (P_total - S)
  ```

`verl/trainer/ppo/metric_utils.py#L306` implements these diagnostics via `compute_variance_proxy_metrics`, emitting `variance_proxy/proxy1_signal_strength`, `variance_proxy/proxy2_total_power`, and `variance_proxy/proxy3_pure_noise`.

Tracking these metrics provides a forward-only, low-overhead view of gradient health for any advantage estimator that supplies `sum_pi_squared`.
